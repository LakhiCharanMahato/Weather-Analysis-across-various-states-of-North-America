{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NY_note1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOPdY0TUdv+zC5S2gwg7i/6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LakhiCharanMahato/Weather-Analysis-across-various-states-of-North-America/blob/master/New%20York/NY_note1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmS-mcH958Ue",
        "colab_type": "text"
      },
      "source": [
        "# Installing Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUGQ_9HbQjgU",
        "colab_type": "text"
      },
      "source": [
        "Ensure that Edit-->Hardware Accelertor(GPU) is selected.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1KSOltQzxnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVWJYq4z0PBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbA3Tnks6Ure",
        "colab_type": "text"
      },
      "source": [
        "# Library Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBHkvHYC8IXj",
        "colab_type": "text"
      },
      "source": [
        "On the left hand side the storage icon is the local storage for Google colab.\n",
        "Make a folder name lib there and upload all the items from Weather-Analysis-across-various-states-of-North-America/New York/lib/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4jLL5CM7bGl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "82d84d3d-523b-4c1a-ea97-cbdba18818e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4hpF9id8zYp",
        "colab_type": "text"
      },
      "source": [
        "# Dealing with nans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzJjQoTT3Jd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "081563b4-d05e-4cbc-e03c-c62bc076cd10"
      },
      "source": [
        "import numpy as np\n",
        "a=np.array([1,np.nan,2,np.nan,3,4,5])\n",
        "print('a=',a)\n",
        "print('np.mean(a)=',np.mean(a))\n",
        "print('np.mean(np.nan_to_num(a))=',np.mean(np.nan_to_num(a))) # =(1+0+2+0+3+4+5)/7\n",
        "print('np.nanmean(a)=',np.nanmean(a)) # =(1+2+3+4+5)/5"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a= [ 1. nan  2. nan  3.  4.  5.]\n",
            "np.mean(a)= nan\n",
            "np.mean(np.nan_to_num(a))= 2.142857142857143\n",
            "np.nanmean(a)= 3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK5mIc3l3prE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "214c97d9-311a-4054-acb3-449a41b6ebff"
      },
      "source": [
        "np.outer(a,a)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1., nan,  2., nan,  3.,  4.,  5.],\n",
              "       [nan, nan, nan, nan, nan, nan, nan],\n",
              "       [ 2., nan,  4., nan,  6.,  8., 10.],\n",
              "       [nan, nan, nan, nan, nan, nan, nan],\n",
              "       [ 3., nan,  6., nan,  9., 12., 15.],\n",
              "       [ 4., nan,  8., nan, 12., 16., 20.],\n",
              "       [ 5., nan, 10., nan, 15., 20., 25.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3HsQT0m9i8O",
        "colab_type": "text"
      },
      "source": [
        "# Computing PCA using RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcCgik8_3wuw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "91197966-dffe-4233-b92b-90ec11b65ad1"
      },
      "source": [
        "import os\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"\n",
        "\n",
        "from pyspark import SparkContext,SparkConf\n",
        "\n",
        "def create_sc(pyFiles):\n",
        "    sc_conf = SparkConf()\n",
        "    sc_conf.setAppName(\"Weather_PCA\")\n",
        "    sc_conf.set('spark.executor.memory', '3g')\n",
        "    sc_conf.set('spark.executor.cores', '1')\n",
        "    sc_conf.set('spark.cores.max', '4')\n",
        "    sc_conf.set('spark.default.parallelism','10')\n",
        "    sc_conf.set('spark.logConf', True)\n",
        "    print(sc_conf.getAll())\n",
        "\n",
        "    sc = SparkContext(conf=sc_conf,pyFiles=pyFiles)\n",
        "\n",
        "    return sc \n",
        "\n",
        "sc = create_sc(pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStatistics.py'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_items([('spark.app.name', 'Weather_PCA'), ('spark.executor.memory', '3g'), ('spark.executor.cores', '1'), ('spark.cores.max', '4'), ('spark.default.parallelism', '10'), ('spark.logConf', 'True')])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2xYoRv03-HR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import *\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "import numpy as np\n",
        "from lib.computeStatistics import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDanH-c74yfS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "09e59341-d621-4c24-9f2e-40f54970ca41"
      },
      "source": [
        "state='NY'\n",
        "EMR=False\n",
        "if not EMR:\n",
        "    data_dir='Data/Weather'\n",
        "    tarname=state+'.tgz'\n",
        "    parquet=state+'.parquet'\n",
        "    %mkdir -p $data_dir\n",
        "\n",
        "    !rm -rf $data_dir/$tarname\n",
        "\n",
        "    command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/by_state/%s > %s/%s\"%(tarname,data_dir,tarname)\n",
        "    print(command)\n",
        "    !$command\n",
        "    !ls -lh $data_dir/$tarname\n",
        "\n",
        "    cur_dir,=!pwd\n",
        "    %cd $data_dir\n",
        "    !tar -xzf $tarname\n",
        "    !du ./$parquet\n",
        "    %cd $cur_dir"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "curl https://mas-dse-open.s3.amazonaws.com/Weather/by_state/NY.tgz > Data/Weather/NY.tgz\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 63.2M  100 63.2M    0     0  12.0M      0  0:00:05  0:00:05 --:--:-- 14.2M\n",
            "-rw-r--r-- 1 root root 64M Mar 24 20:32 Data/Weather/NY.tgz\n",
            "/content/Data/Weather\n",
            "77828\t./NY.parquet\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-u12YkJBb2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if EMR:  # not debugged, should use complete parquet and extract just the state of interest.\n",
        "    data_dir='/mnt/workspace/Data'\n",
        "    !hdfs dfs -mkdir /weather/\n",
        "    !hdfs dfs -CopyFromLocal $data_dir/$parquet /weather/$parquet\n",
        "\n",
        "    # When running on cluster\n",
        "    #!mv ../../Data/Weather/NY.parquet /mnt/workspace/Data/NY.parquet\n",
        "\n",
        "    !aws s3 cp --recursive --quiet /mnt/workspace/Data/NY.parquet s3://dse-weather/NY.parquet\n",
        "\n",
        "    !aws s3 ls s3://dse-weather/\n",
        "\n",
        "    local_path=data_dir+'/'+parquet\n",
        "    hdfs_path='/weather/'+parquet\n",
        "    local_path,hdfs_path\n",
        "\n",
        "    !hdfs dfs -copyFromLocal $local_path $hdfs_path\n",
        "\n",
        "    !hdfs dfs -du /weather/\n",
        "    parquet_path=hdfs_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_G6ksSkAyEL",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21djRanz5D0W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d9fecf4-9b5f-4563-d985-64560eb24020"
      },
      "source": [
        "parquet_path = data_dir+'/'+parquet\n",
        "!du -sh $parquet_path"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "77M\tData/Weather/NY.parquet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysBAz7rIBpwF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "9f529fa8-fd28-4f23-ad37-c0f35d562bd0"
      },
      "source": [
        "%%time\n",
        "df=sqlContext.read.parquet(parquet_path)\n",
        "print(df.count())\n",
        "df.show(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "168398\n",
            "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
            "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
            "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
            "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
            "|USW00094704|   PRCP_s20|1946|[99 46 52 46 0B 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
            "|USW00094704|   PRCP_s20|1947|[79 4C 75 4C 8F 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
            "|USW00094704|   PRCP_s20|1948|[72 48 7A 48 85 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
            "|USW00094704|   PRCP_s20|1949|[BB 49 BC 49 BD 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
            "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "CPU times: user 5.05 ms, sys: 1.52 ms, total: 6.56 ms\n",
            "Wall time: 5.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVDFNMyqBuTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqlContext.registerDataFrameAsTable(df,'table')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WWC9RvqB4m-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "38675c95-10b5-458c-f955-d78054a6970e"
      },
      "source": [
        "Query=\"\"\"\n",
        "SELECT Measurement,count(Measurement) as count \n",
        "FROM table \n",
        "GROUP BY Measurement\n",
        "ORDER BY count\n",
        "\"\"\"\n",
        "counts=sqlContext.sql(Query)\n",
        "counts.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-----+\n",
            "|Measurement|count|\n",
            "+-----------+-----+\n",
            "|   TOBS_s20|10956|\n",
            "|       TOBS|10956|\n",
            "|   TMAX_s20|13437|\n",
            "|       TMAX|13437|\n",
            "|       TMIN|13442|\n",
            "|   TMIN_s20|13442|\n",
            "|   SNWD_s20|14617|\n",
            "|       SNWD|14617|\n",
            "|       SNOW|15629|\n",
            "|   SNOW_s20|15629|\n",
            "|   PRCP_s20|16118|\n",
            "|       PRCP|16118|\n",
            "+-----------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkN9uhMEB-Sx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "657193f0-450a-4bc9-efc3-ea9bc795c254"
      },
      "source": [
        "from time import time\n",
        "t=time()\n",
        "\n",
        "N=sc.defaultParallelism\n",
        "print('Number of executors=',N)\n",
        "print('took',time()-t,'seconds')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of executors= 10\n",
            "took 0.004098415374755859 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc0n0vxzCG16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f27d1e2f-3122-449e-fa41-e4741d1d3ae2"
      },
      "source": [
        "!ls lib"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "computeStatistics.py  numpy_pack.py  Reconstruction_plots.py  YearPlotter.py\n",
            "decomposer.py\t      __pycache__    spark_PCA.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jz6XjxlCN1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load lib/spark_PCA.py\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "\n",
        "def outerProduct(X):\n",
        "    \"\"\"Computer outer product and indicate which locations in matrix are undefined\"\"\"\n",
        "    O=np.outer(X,X)\n",
        "    N=1-np.isnan(O)\n",
        "    return (O,N)\n",
        "\n",
        "def sumWithNan(M1,M2):\n",
        "    \"\"\"Add two pairs of (matrix,count)\"\"\"\n",
        "    (X1,N1)=M1\n",
        "    (X2,N2)=M2\n",
        "    N=N1+N2\n",
        "    X=np.nansum(np.dstack((X1,X2)),axis=2)\n",
        "    return (X,N)\n",
        "\n",
        "def HW_func(S,N):\n",
        "    E= S[1:,0]                              # E is the sum of the vectors\n",
        "    NE= np.array(N[1:,0],dtype=np.float)    # NE is the number of not-nan antries for each coordinate of the vectors\n",
        "    Mean= E/NE                              # Mean is the Mean vector (ignoring nans)\n",
        "    O= S[1:,1:]                             # O is the sum of the outer products\n",
        "    NO= np.array(N[1:,1:],dtype=np.float)   # NO is the number of non-nans in the outer product.\n",
        "    return  E,NE,Mean,O,NO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAy9_6YCD4pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeCov(RDDin):\n",
        "    \"\"\"computeCov recieves as input an RDD of np arrays, all of the same length, \n",
        "    and computes the covariance matrix for that set of vectors\"\"\"\n",
        "    RDD=RDDin.map(lambda v:np.array(np.insert(v,0,1),dtype=np.float64)) # insert a 1 at the beginning of each vector so that the same \n",
        "                                           #calculation also yields the mean vector\n",
        "    OuterRDD=RDD.map(outerProduct)   # separating the map and the reduce does not matter because of Spark uses lazy execution.\n",
        "    (S,N)=OuterRDD.reduce(sumWithNan)\n",
        "\n",
        "    E,NE,Mean,O,NO=HW_func(S,N)\n",
        "\n",
        "    Cov=O/NO - np.outer(Mean,Mean)\n",
        "    # Output also the diagnal which is the variance for each day\n",
        "    Var=np.array([Cov[i,i] for i in range(Cov.shape[0])])\n",
        "    return {'E':E,'NE':NE,'O':O,'NO':NO,'Cov':Cov,'Mean':Mean,'Var':Var}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f_Su0g5EFzX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "outputId": "77b732d3-c904-41f0-fae9-66425cca6abc"
      },
      "source": [
        "if __name__==\"__main__\":\n",
        "    # create synthetic data matrix with j rows and rank k\n",
        "    \n",
        "    V=2*(np.random.random([2,10])-0.5)\n",
        "    data_list=[]\n",
        "    for i in range(1000):\n",
        "        f=2*(np.random.random(2)-0.5)\n",
        "        data_list.append(np.dot(f,V))\n",
        "    # compute covariance matrix\n",
        "    RDD=sc.parallelize(data_list)\n",
        "    OUT=computeCov(RDD)\n",
        "\n",
        "    #find PCA decomposition\n",
        "    eigval,eigvec=LA.eig(OUT['Cov'])\n",
        "    print('eigval=',eigval)\n",
        "    print('eigvec=',eigvec)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eigval= [ 0.00000000e+00+0.00000000e+00j  7.96272650e-01+0.00000000e+00j\n",
            "  1.43441595e+00+0.00000000e+00j -1.17835774e-16+0.00000000e+00j\n",
            " -7.68323266e-17+0.00000000e+00j  7.03447951e-17+2.73777728e-17j\n",
            "  7.03447951e-17-2.73777728e-17j  5.30894870e-17+0.00000000e+00j\n",
            " -1.92559518e-18+0.00000000e+00j  1.08871857e-18+0.00000000e+00j]\n",
            "eigvec= [[-8.33605610e-01+0.j         -5.51564872e-01+0.j\n",
            "   2.96290357e-02+0.j          7.55191775e-04+0.j\n",
            "   1.62980498e-01+0.j         -3.03934129e-01-0.04827158j\n",
            "  -3.03934129e-01+0.04827158j -3.20528479e-02+0.j\n",
            "  -7.44119631e-01+0.j          8.28543198e-01+0.j        ]\n",
            " [-3.76215657e-01+0.j          5.78688227e-01+0.j\n",
            "   1.87944543e-01+0.j         -1.65743727e-01+0.j\n",
            "   1.09021200e-01+0.j         -4.40671543e-01+0.j\n",
            "  -4.40671543e-01-0.j         -6.47647089e-02+0.j\n",
            "  -3.54129060e-01+0.j          3.80096252e-01+0.j        ]\n",
            " [ 2.33620628e-01+0.j         -3.38926219e-01+0.j\n",
            "   2.63514149e-01+0.j          1.68777346e-01+0.j\n",
            "   2.52908879e-02+0.j         -1.91516441e-02-0.01146414j\n",
            "  -1.91516441e-02+0.01146414j -1.83113803e-01+0.j\n",
            "   1.87119803e-01+0.j         -2.38041479e-01+0.j        ]\n",
            " [ 1.18875325e-01+0.j         -2.09201323e-01+0.j\n",
            "  -5.49898527e-01+0.j          2.75834257e-02+0.j\n",
            "  -2.06427785e-01+0.j          5.68507471e-02+0.25037418j\n",
            "   5.68507471e-02-0.25037418j -3.32122653e-01+0.j\n",
            "   1.03480566e-01+0.j         -1.14704096e-01+0.j        ]\n",
            " [ 2.47841952e-01+0.j         -3.59507535e-01+0.j\n",
            "   2.80492239e-01+0.j         -8.25925102e-02+0.j\n",
            "  -2.43396200e-01+0.j         -5.89238789e-02-0.16465947j\n",
            "  -5.89238789e-02+0.16465947j  1.26176130e-01+0.j\n",
            "   3.23630047e-01+0.j         -2.67112508e-01+0.j        ]\n",
            " [ 3.05272832e-02+0.j         -4.26040215e-02+0.j\n",
            "   6.57744265e-02+0.j         -4.78492706e-02+0.j\n",
            "   3.54254253e-02+0.j          5.73253049e-02-0.05843193j\n",
            "   5.73253049e-02+0.05843193j  1.12256049e-03+0.j\n",
            "   3.91245297e-01+0.j          6.82501281e-02+0.j        ]\n",
            " [ 5.44910612e-02+0.j         -6.58969298e-02+0.j\n",
            "   3.06375906e-01+0.j         -1.95847877e-01+0.j\n",
            "   1.64104992e-01+0.j          2.66223489e-01-0.3042417j\n",
            "   2.66223489e-01+0.3042417j  -7.50459626e-01+0.j\n",
            "  -9.55250476e-02+0.j         -3.24424438e-02+0.j        ]\n",
            " [ 8.45800262e-02+0.j         -1.40409037e-01+0.j\n",
            "  -2.34172604e-01+0.j          3.50424818e-01+0.j\n",
            "  -3.96475242e-01+0.j         -3.38218057e-01+0.10516711j\n",
            "  -3.38218057e-01-0.10516711j  3.77648073e-01+0.j\n",
            "  -8.38382618e-02+0.j         -4.75835132e-02+0.j        ]\n",
            " [-3.63618406e-02+0.j          8.16438669e-02+0.j\n",
            "   4.96825306e-01+0.j          7.01691434e-01+0.j\n",
            "  -7.17906804e-01+0.j          8.51319380e-02+0.3701687j\n",
            "   8.51319380e-02-0.3701687j   8.92945039e-02+0.j\n",
            "  -3.13400158e-02+0.j          3.76096402e-02+0.j        ]\n",
            " [ 1.45088723e-01+0.j         -2.01187372e-01+0.j\n",
            "   3.36794168e-01+0.j         -5.29760249e-01+0.j\n",
            "   3.97856455e-01+0.j         -1.84588876e-01+0.37438602j\n",
            "  -1.84588876e-01-0.37438602j  3.48194091e-01+0.j\n",
            "   1.45313597e-02+0.j         -1.35955409e-01+0.j        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhKx9HswENZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "43b59729-e2a7-4317-e6f8-492ec8210e4d"
      },
      "source": [
        "%%writefile lib/tmp\n",
        "# %load lib/computeStatistics.py\n",
        "\n",
        "\n",
        "from numpy import linalg as LA\n",
        "import numpy as np\n",
        "\n",
        "from numpy_pack import packArray,unpackArray\n",
        "from spark_PCA import computeCov\n",
        "from time import time\n",
        "\n",
        "def computeStatistics(sqlContext,df):\n",
        "    \"\"\"Compute all of the statistics for a given dataframe\n",
        "    Input: sqlContext: to perform SQL queries\n",
        "            df: dataframe with the fields \n",
        "            Station(string), Measurement(string), Year(integer), Values (byteArray with 365 float16 numbers)\n",
        "    returns: STAT, a dictionary of dictionaries. First key is measurement, \n",
        "             second keys described in computeStats.STAT_Descriptions\n",
        "    \"\"\"\n",
        "\n",
        "    sqlContext.registerDataFrameAsTable(df,'weather')\n",
        "    STAT={}  # dictionary storing the statistics for each measurement\n",
        "    measurements=['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']\n",
        "    \n",
        "    for meas in measurements:\n",
        "        t=time()\n",
        "        Query=\"SELECT * FROM weather\\n\\tWHERE measurement = '%s'\"%(meas)\n",
        "        mdf = sqlContext.sql(Query)\n",
        "        print(meas,': shape of mdf is ',mdf.count())\n",
        "\n",
        "        data=mdf.rdd.map(lambda row: unpackArray(row['Values'],np.float16))\n",
        "\n",
        "        #Compute basic statistics\n",
        "        STAT[meas]=computeOverAllDist(data)   # Compute the statistics \n",
        "\n",
        "        # compute covariance matrix\n",
        "        OUT=computeCov(data)\n",
        "\n",
        "        #find PCA decomposition\n",
        "        eigval,eigvec=LA.eig(OUT['Cov'])\n",
        "\n",
        "        # collect all of the statistics in STAT[meas]\n",
        "        STAT[meas]['eigval']=eigval\n",
        "        STAT[meas]['eigvec']=eigvec\n",
        "        STAT[meas].update(OUT)\n",
        "\n",
        "        print('time for',meas,'is',time()-t)\n",
        "    \n",
        "    return STAT\n",
        "\n",
        "# Compute the overall distribution of values and the distribution of the number of nan per year\n",
        "def find_percentiles(SortedVals,percentile):\n",
        "    L=int(len(SortedVals)/percentile)\n",
        "    return SortedVals[L],SortedVals[-L]\n",
        "  \n",
        "def computeOverAllDist(rdd0):\n",
        "    UnDef=np.array(rdd0.map(lambda row:sum(np.isnan(row))).sample(False,0.01).collect())\n",
        "    flat=rdd0.flatMap(lambda v:list(v)).filter(lambda x: not np.isnan(x)).cache()\n",
        "    count,S1,S2=flat.map(lambda x: np.float64([1,x,x**2]))\\\n",
        "                  .reduce(lambda x,y: x+y)\n",
        "    mean=S1/count\n",
        "    std=np.sqrt(S2/count-mean**2)\n",
        "    Vals=flat.sample(False,0.0001).collect()\n",
        "    SortedVals=np.array(sorted(Vals))\n",
        "    low100,high100=find_percentiles(SortedVals,100)\n",
        "    low1000,high1000=find_percentiles(SortedVals,1000)\n",
        "    return {'UnDef':UnDef,\\\n",
        "          'mean':mean,\\\n",
        "          'std':std,\\\n",
        "          'SortedVals':SortedVals,\\\n",
        "          'low100':low100,\\\n",
        "          'high100':high100,\\\n",
        "          'low1000':low100,\\\n",
        "          'high1000':high1000\n",
        "          }\n",
        "\n",
        "# description of data returned by computeOverAllDist\n",
        "STAT_Descriptions=[\n",
        "('SortedVals', 'Sample of values', 'vector whose length varies between measurements'),\n",
        " ('UnDef', 'sample of number of undefs per row', 'vector whose length varies between measurements'),\n",
        " ('mean', 'mean value', ()),\n",
        " ('std', 'std', ()),\n",
        " ('low100', 'bottom 1%', ()),\n",
        " ('high100', 'top 1%', ()),\n",
        " ('low1000', 'bottom 0.1%', ()),\n",
        " ('high1000', 'top 0.1%', ()),\n",
        " ('E', 'Sum of values per day', (365,)),\n",
        " ('NE', 'count of values per day', (365,)),\n",
        " ('Mean', 'E/NE', (365,)),\n",
        " ('O', 'Sum of outer products', (365, 365)),\n",
        " ('NO', 'counts for outer products', (365, 365)),\n",
        " ('Cov', 'O/NO', (365, 365)),\n",
        " ('Var', 'The variance per day = diagonal of Cov', (365,)),\n",
        " ('eigval', 'PCA eigen-values', (365,)),\n",
        " ('eigvec', 'PCA eigen-vectors', (365, 365))\n",
        "]\n",
        "\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing lib/tmp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS9RALI_Ehn8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "e67285f3-5a42-4b67-8d5e-8b66f5774f64"
      },
      "source": [
        "%%time \n",
        "### This is the main cell, where all of the statistics are computed.\n",
        "STAT=computeStatistics(sqlContext,df)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TMAX : shape of mdf is  13437\n",
            "time for TMAX is 102.15825271606445\n",
            "SNOW : shape of mdf is  15629\n",
            "time for SNOW is 113.16211414337158\n",
            "SNWD : shape of mdf is  14617\n",
            "time for SNWD is 102.48482179641724\n",
            "TMIN : shape of mdf is  13442\n",
            "time for TMIN is 97.81967091560364\n",
            "PRCP : shape of mdf is  16118\n",
            "time for PRCP is 117.40839314460754\n",
            "TOBS : shape of mdf is  10956\n",
            "time for TOBS is 81.81937742233276\n",
            "CPU times: user 525 ms, sys: 583 ms, total: 1.11 s\n",
            "Wall time: 10min 14s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVnWI2GlEqlU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "155db6aa-bf0a-417f-c72b-dd3ee01e3678"
      },
      "source": [
        "print(\"   Name  \\t                 Description             \\t  Size\")\n",
        "print(\"-\"*80)\n",
        "print('\\n'.join([\"%10s\\t%40s\\t%s\"%(s[0],s[1],str(s[2])) for s in STAT_Descriptions]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Name  \t                 Description             \t  Size\n",
            "--------------------------------------------------------------------------------\n",
            "SortedVals\t                        Sample of values\tvector whose length varies between measurements\n",
            "     UnDef\t      sample of number of undefs per row\tvector whose length varies between measurements\n",
            "      mean\t                              mean value\t()\n",
            "       std\t                                     std\t()\n",
            "    low100\t                               bottom 1%\t()\n",
            "   high100\t                                  top 1%\t()\n",
            "   low1000\t                             bottom 0.1%\t()\n",
            "  high1000\t                                top 0.1%\t()\n",
            "         E\t                   Sum of values per day\t(365,)\n",
            "        NE\t                 count of values per day\t(365,)\n",
            "      Mean\t                                    E/NE\t(365,)\n",
            "         O\t                   Sum of outer products\t(365, 365)\n",
            "        NO\t               counts for outer products\t(365, 365)\n",
            "       Cov\t                                    O/NO\t(365, 365)\n",
            "       Var\t  The variance per day = diagonal of Cov\t(365,)\n",
            "    eigval\t                        PCA eigen-values\t(365,)\n",
            "    eigvec\t                       PCA eigen-vectors\t(365, 365)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPDT8N1HJHrS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "bc9f5085-c75e-44c6-d68a-42896fd76dd8"
      },
      "source": [
        "## Dump STAT and STST_Descriptions into a pickle file.\n",
        "from pickle import dump\n",
        "\n",
        "filename=data_dir+'/STAT_%s.pickle'%state\n",
        "dump((STAT,STAT_Descriptions),open(filename,'wb'))\n",
        "!ls -l $data_dir"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 89824\n",
            "drwxrwxr-x 2  498  500     4096 Apr 19  2018 NY.parquet\n",
            "-rw-r--r-- 1 root root 66288146 Mar 24 20:32 NY.tgz\n",
            "-rw-r--r-- 1 root root 25684462 Mar 24 20:43 STAT_NY.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}