{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NY_note1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPd1OEA8niIGeLpHDCxXYjI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LakhiCharanMahato/Weather-Analysis-across-various-states-of-North-America/blob/master/New%20York/NY_note1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmS-mcH958Ue",
        "colab_type": "text"
      },
      "source": [
        "# Installing Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUGQ_9HbQjgU",
        "colab_type": "text"
      },
      "source": [
        "**Ensure that Edit-->Hardware Accelertor(GPU) is enabled.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1KSOltQzxnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVWJYq4z0PBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbA3Tnks6Ure",
        "colab_type": "text"
      },
      "source": [
        "# Library Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBHkvHYC8IXj",
        "colab_type": "text"
      },
      "source": [
        "**On the left hand side the storage icon is the local storage for Google colab.\n",
        "Make a folder name lib there and upload all the items from Weather-Analysis-across-various-states-of-North-America/New York/lib/**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4jLL5CM7bGl",
        "colab_type": "code",
        "outputId": "85d98edd-5091-448d-be0e-2d85210aaed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4hpF9id8zYp",
        "colab_type": "text"
      },
      "source": [
        "# Dealing with nans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzJjQoTT3Jd_",
        "colab_type": "code",
        "outputId": "7a4031d5-37f3-4370-f47f-bce9d829422f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import numpy as np\n",
        "a=np.array([1,np.nan,2,np.nan,3,4,5])\n",
        "print('a=',a)\n",
        "print('np.mean(a)=',np.mean(a))\n",
        "print('np.mean(np.nan_to_num(a))=',np.mean(np.nan_to_num(a))) # =(1+0+2+0+3+4+5)/7\n",
        "print('np.nanmean(a)=',np.nanmean(a)) # =(1+2+3+4+5)/5"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a= [ 1. nan  2. nan  3.  4.  5.]\n",
            "np.mean(a)= nan\n",
            "np.mean(np.nan_to_num(a))= 2.142857142857143\n",
            "np.nanmean(a)= 3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK5mIc3l3prE",
        "colab_type": "code",
        "outputId": "2dd2bfbd-4b35-48c0-f825-b221d1e69f52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "np.outer(a,a)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1., nan,  2., nan,  3.,  4.,  5.],\n",
              "       [nan, nan, nan, nan, nan, nan, nan],\n",
              "       [ 2., nan,  4., nan,  6.,  8., 10.],\n",
              "       [nan, nan, nan, nan, nan, nan, nan],\n",
              "       [ 3., nan,  6., nan,  9., 12., 15.],\n",
              "       [ 4., nan,  8., nan, 12., 16., 20.],\n",
              "       [ 5., nan, 10., nan, 15., 20., 25.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3HsQT0m9i8O",
        "colab_type": "text"
      },
      "source": [
        "# Computing PCA using RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0v3HecXH_cy",
        "colab_type": "text"
      },
      "source": [
        "Computing PCA involves 2 steps:\n",
        "1. Computing covariance matrix.\n",
        "2. Computing the eigen vector decomposition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcCgik8_3wuw",
        "colab_type": "code",
        "outputId": "ad7718df-89fa-4a23-d784-783cadcbc9d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import os\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"\n",
        "\n",
        "from pyspark import SparkContext,SparkConf\n",
        "\n",
        "def create_sc(pyFiles):\n",
        "    sc_conf = SparkConf()\n",
        "    sc_conf.setAppName(\"Weather_PCA\")\n",
        "    sc_conf.set('spark.executor.memory', '3g')\n",
        "    sc_conf.set('spark.executor.cores', '1')\n",
        "    sc_conf.set('spark.cores.max', '4')\n",
        "    sc_conf.set('spark.default.parallelism','10')\n",
        "    sc_conf.set('spark.logConf', True)\n",
        "    print(sc_conf.getAll())\n",
        "\n",
        "    sc = SparkContext(conf=sc_conf,pyFiles=pyFiles)\n",
        "\n",
        "    return sc \n",
        "\n",
        "sc = create_sc(pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStatistics.py'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_items([('spark.app.name', 'Weather_PCA'), ('spark.executor.memory', '3g'), ('spark.executor.cores', '1'), ('spark.cores.max', '4'), ('spark.default.parallelism', '10'), ('spark.logConf', 'True')])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2xYoRv03-HR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import *\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "import numpy as np\n",
        "from lib.computeStatistics import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDanH-c74yfS",
        "colab_type": "code",
        "outputId": "cd4dfa45-6ca7-4775-c1c0-d75a8b709c97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "state='NY'\n",
        "EMR=False\n",
        "if not EMR:\n",
        "    data_dir='Data/Weather'\n",
        "    tarname=state+'.tgz'\n",
        "    parquet=state+'.parquet'\n",
        "    %mkdir -p $data_dir\n",
        "\n",
        "    !rm -rf $data_dir/$tarname\n",
        "\n",
        "    command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/by_state/%s > %s/%s\"%(tarname,data_dir,tarname)\n",
        "    print(command)\n",
        "    !$command\n",
        "    !ls -lh $data_dir/$tarname\n",
        "\n",
        "    cur_dir,=!pwd\n",
        "    %cd $data_dir\n",
        "    !tar -xzf $tarname\n",
        "    !du ./$parquet\n",
        "    %cd $cur_dir"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "curl https://mas-dse-open.s3.amazonaws.com/Weather/by_state/NY.tgz > Data/Weather/NY.tgz\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 63.2M  100 63.2M    0     0  12.8M      0  0:00:04  0:00:04 --:--:-- 15.2M\n",
            "-rw-r--r-- 1 root root 64M Mar 26 09:27 Data/Weather/NY.tgz\n",
            "/content/Data/Weather\n",
            "77828\t./NY.parquet\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-u12YkJBb2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if EMR:  # not debugged, should use complete parquet and extract just the state of interest.\n",
        "    data_dir='/mnt/workspace/Data'\n",
        "    !hdfs dfs -mkdir /weather/\n",
        "    !hdfs dfs -CopyFromLocal $data_dir/$parquet /weather/$parquet\n",
        "\n",
        "    # When running on cluster\n",
        "    #!mv ../../Data/Weather/NY.parquet /mnt/workspace/Data/NY.parquet\n",
        "\n",
        "    !aws s3 cp --recursive --quiet /mnt/workspace/Data/NY.parquet s3://dse-weather/NY.parquet\n",
        "\n",
        "    !aws s3 ls s3://dse-weather/\n",
        "\n",
        "    local_path=data_dir+'/'+parquet\n",
        "    hdfs_path='/weather/'+parquet\n",
        "    local_path,hdfs_path\n",
        "\n",
        "    !hdfs dfs -copyFromLocal $local_path $hdfs_path\n",
        "\n",
        "    !hdfs dfs -du /weather/\n",
        "    parquet_path=hdfs_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_G6ksSkAyEL",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21djRanz5D0W",
        "colab_type": "code",
        "outputId": "5fcf9d14-c198-4250-bb56-d9bb97e69e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "parquet_path = data_dir+'/'+parquet\n",
        "!du -sh $parquet_path"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "77M\tData/Weather/NY.parquet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysBAz7rIBpwF",
        "colab_type": "code",
        "outputId": "f99dd668-228d-4b9c-d93c-2ebe87d31c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "%%time\n",
        "df=sqlContext.read.parquet(parquet_path)\n",
        "print(df.count())\n",
        "df.show(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "168398\n",
            "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
            "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
            "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
            "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
            "|USW00094704|   PRCP_s20|1946|[99 46 52 46 0B 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
            "|USW00094704|   PRCP_s20|1947|[79 4C 75 4C 8F 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
            "|USW00094704|   PRCP_s20|1948|[72 48 7A 48 85 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
            "|USW00094704|   PRCP_s20|1949|[BB 49 BC 49 BD 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
            "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "CPU times: user 5 ms, sys: 2.76 ms, total: 7.75 ms\n",
            "Wall time: 6.98 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVDFNMyqBuTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqlContext.registerDataFrameAsTable(df,'table')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WWC9RvqB4m-",
        "colab_type": "code",
        "outputId": "445e2ff6-c407-48a6-9fa6-b33c9c8456ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "Query=\"\"\"\n",
        "SELECT Measurement,count(Measurement) as count \n",
        "FROM table \n",
        "GROUP BY Measurement\n",
        "ORDER BY count\n",
        "\"\"\"\n",
        "counts=sqlContext.sql(Query)\n",
        "counts.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-----+\n",
            "|Measurement|count|\n",
            "+-----------+-----+\n",
            "|   TOBS_s20|10956|\n",
            "|       TOBS|10956|\n",
            "|   TMAX_s20|13437|\n",
            "|       TMAX|13437|\n",
            "|   TMIN_s20|13442|\n",
            "|       TMIN|13442|\n",
            "|   SNWD_s20|14617|\n",
            "|       SNWD|14617|\n",
            "|       SNOW|15629|\n",
            "|   SNOW_s20|15629|\n",
            "|   PRCP_s20|16118|\n",
            "|       PRCP|16118|\n",
            "+-----------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkN9uhMEB-Sx",
        "colab_type": "code",
        "outputId": "be4b3c6e-f8e2-4b85-9a9d-b8946a0ffb6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from time import time\n",
        "t=time()\n",
        "\n",
        "N=sc.defaultParallelism\n",
        "print('Number of executors=',N)\n",
        "print('took',time()-t,'seconds')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of executors= 10\n",
            "took 0.0050945281982421875 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc0n0vxzCG16",
        "colab_type": "code",
        "outputId": "8c7d58ad-2d40-44aa-e84b-f9d4bfc301e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!ls lib"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "computeStatistics.py  MultiPlot.py   __pycache__\t      spark_PCA.py\n",
            "decomposer.py\t      numpy_pack.py  Reconstruction_plots.py  YearPlotter.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_84XpdDI18D",
        "colab_type": "text"
      },
      "source": [
        "# Computing Covariance Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jz6XjxlCN1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load lib/spark_PCA.py\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "\n",
        "def outerProduct(X):\n",
        "    \"\"\"Computer outer product and indicate which locations in matrix are undefined\"\"\"\n",
        "    O=np.outer(X,X)\n",
        "    N=1-np.isnan(O)\n",
        "    return (O,N)\n",
        "\n",
        "def sumWithNan(M1,M2):\n",
        "    \"\"\"Add two pairs of (matrix,count)\"\"\"\n",
        "    (X1,N1)=M1\n",
        "    (X2,N2)=M2\n",
        "    N=N1+N2\n",
        "    X=np.nansum(np.dstack((X1,X2)),axis=2)\n",
        "    return (X,N)\n",
        "\n",
        "def HW_func(S,N):\n",
        "    E= S[1:,0]                              # E is the sum of the vectors\n",
        "    NE= np.array(N[1:,0],dtype=np.float)    # NE is the number of not-nan antries for each coordinate of the vectors\n",
        "    Mean= E/NE                              # Mean is the Mean vector (ignoring nans)\n",
        "    O= S[1:,1:]                             # O is the sum of the outer products\n",
        "    NO= np.array(N[1:,1:],dtype=np.float)   # NO is the number of non-nans in the outer product.\n",
        "    return  E,NE,Mean,O,NO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAy9_6YCD4pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeCov(RDDin):\n",
        "    \"\"\"computeCov recieves as input an RDD of np arrays, all of the same length, \n",
        "    and computes the covariance matrix for that set of vectors\"\"\"\n",
        "    RDD=RDDin.map(lambda v:np.array(np.insert(v,0,1),dtype=np.float64)) # insert a 1 at the beginning of each vector so that the same \n",
        "                                           #calculation also yields the mean vector\n",
        "    OuterRDD=RDD.map(outerProduct)   # separating the map and the reduce does not matter because of Spark uses lazy execution.\n",
        "    (S,N)=OuterRDD.reduce(sumWithNan)\n",
        "\n",
        "    E,NE,Mean,O,NO=HW_func(S,N)\n",
        "\n",
        "    Cov=O/NO - np.outer(Mean,Mean)\n",
        "    # Output also the diagnal which is the variance for each day\n",
        "    Var=np.array([Cov[i,i] for i in range(Cov.shape[0])])\n",
        "    return {'E':E,'NE':NE,'O':O,'NO':NO,'Cov':Cov,'Mean':Mean,'Var':Var}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f_Su0g5EFzX",
        "colab_type": "code",
        "outputId": "173b4955-57fc-4ca7-b3ab-d23a44450d85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        }
      },
      "source": [
        "if __name__==\"__main__\":\n",
        "    # create synthetic data matrix with j rows and rank k\n",
        "    \n",
        "    V=2*(np.random.random([2,10])-0.5)\n",
        "    data_list=[]\n",
        "    for i in range(1000):\n",
        "        f=2*(np.random.random(2)-0.5)\n",
        "        data_list.append(np.dot(f,V))\n",
        "    # compute covariance matrix\n",
        "    RDD=sc.parallelize(data_list)\n",
        "    OUT=computeCov(RDD)\n",
        "\n",
        "    #find PCA decomposition\n",
        "    eigval,eigvec=LA.eig(OUT['Cov'])\n",
        "    print('eigval=',eigval)\n",
        "    print('eigvec=',eigvec)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eigval= [ 1.36685487e+00+0.00000000e+00j  2.29367848e-01+0.00000000e+00j\n",
            " -8.10105924e-17+7.96382746e-17j -8.10105924e-17-7.96382746e-17j\n",
            "  7.88851209e-17+0.00000000e+00j  5.39699625e-17+0.00000000e+00j\n",
            " -3.67980520e-17+0.00000000e+00j  1.58193311e-17+7.41164318e-18j\n",
            "  1.58193311e-17-7.41164318e-18j -6.36689236e-18+0.00000000e+00j]\n",
            "eigvec= [[-0.35780548+0.j         -0.01518794+0.j          0.38379663-0.23286603j\n",
            "   0.38379663+0.23286603j  0.458165  +0.j         -0.1562244 +0.j\n",
            "   0.02236314+0.j         -0.0263539 +0.19156201j -0.0263539 -0.19156201j\n",
            "  -0.06512137+0.j        ]\n",
            " [ 0.16635764+0.j         -0.18593124+0.j          0.64604164+0.j\n",
            "   0.64604164-0.j         -0.52884859+0.j          0.18917869+0.j\n",
            "  -0.08050918+0.j         -0.2490146 -0.14885252j -0.2490146 +0.14885252j\n",
            "  -0.24879068+0.j        ]\n",
            " [ 0.10596097+0.j          0.41570014+0.j          0.16368015-0.02437838j\n",
            "   0.16368015+0.02437838j  0.25671615+0.j          0.15271127+0.j\n",
            "  -0.38804382+0.j          0.06913741-0.09676985j  0.06913741+0.09676985j\n",
            "  -0.40582068+0.j        ]\n",
            " [ 0.44150484+0.j          0.29656535+0.j         -0.06806029-0.14886803j\n",
            "  -0.06806029+0.14886803j -0.15306983+0.j         -0.5502214 +0.j\n",
            "  -0.08930217+0.j          0.1570551 +0.18692471j  0.1570551 -0.18692471j\n",
            "   0.07925149+0.j        ]\n",
            " [-0.08212213+0.j         -0.63887801+0.j         -0.11926585-0.22959007j\n",
            "  -0.11926585+0.22959007j -0.17785548+0.j         -0.00785623+0.j\n",
            "  -0.54354393+0.j         -0.15060887+0.06688904j -0.15060887-0.06688904j\n",
            "  -0.0695969 +0.j        ]\n",
            " [ 0.29401227+0.j         -0.09025922+0.j         -0.07469078-0.03646038j\n",
            "  -0.07469078+0.03646038j -0.08271777+0.j          0.60706534+0.j\n",
            "   0.31794429+0.j         -0.31726533+0.2394127j  -0.31726533-0.2394127j\n",
            "  -0.44406357+0.j        ]\n",
            " [ 0.24392837+0.j         -0.2084341 +0.j          0.17029006+0.10141183j\n",
            "   0.17029006-0.10141183j  0.28165927+0.j          0.13585959+0.j\n",
            "  -0.30031791+0.j          0.61172575+0.j          0.61172575-0.j\n",
            "   0.65707751+0.j        ]\n",
            " [ 0.37151761+0.j          0.23965406+0.j          0.01777968-0.34900525j\n",
            "   0.01777968+0.34900525j -0.18694138+0.j          0.33597836+0.j\n",
            "  -0.11083491+0.j         -0.25691898-0.14275875j -0.25691898+0.14275875j\n",
            "   0.15593082+0.j        ]\n",
            " [-0.24033639+0.j          0.36368856+0.j          0.11294941+0.15901405j\n",
            "   0.11294941-0.15901405j -0.14819024+0.j          0.16962756+0.j\n",
            "  -0.56890952+0.j         -0.25265726+0.24162348j -0.25265726-0.24162348j\n",
            "   0.32313587+0.j        ]\n",
            " [-0.53821083+0.j          0.23448079+0.j         -0.0626581 -0.22295163j\n",
            "  -0.0626581 +0.22295163j -0.4963429 +0.j          0.2916131 +0.j\n",
            "   0.10864363+0.j          0.14538733-0.12493516j  0.14538733+0.12493516j\n",
            "  -0.01931174+0.j        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5VSHaaaJERs",
        "colab_type": "text"
      },
      "source": [
        "# Computing the eigen vector decomposition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhKx9HswENZJ",
        "colab_type": "code",
        "outputId": "c7b0746f-15de-4a1f-a363-ef6eab976340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile lib/tmp\n",
        "# %load lib/computeStatistics.py\n",
        "\n",
        "\n",
        "from numpy import linalg as LA\n",
        "import numpy as np\n",
        "\n",
        "from numpy_pack import packArray,unpackArray\n",
        "from spark_PCA import computeCov\n",
        "from time import time\n",
        "\n",
        "def computeStatistics(sqlContext,df):\n",
        "    \"\"\"Compute all of the statistics for a given dataframe\n",
        "    Input: sqlContext: to perform SQL queries\n",
        "            df: dataframe with the fields \n",
        "            Station(string), Measurement(string), Year(integer), Values (byteArray with 365 float16 numbers)\n",
        "    returns: STAT, a dictionary of dictionaries. First key is measurement, \n",
        "             second keys described in computeStats.STAT_Descriptions\n",
        "    \"\"\"\n",
        "\n",
        "    sqlContext.registerDataFrameAsTable(df,'weather')\n",
        "    STAT={}  # dictionary storing the statistics for each measurement\n",
        "    measurements=['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']\n",
        "    \n",
        "    for meas in measurements:\n",
        "        t=time()\n",
        "        Query=\"SELECT * FROM weather\\n\\tWHERE measurement = '%s'\"%(meas)\n",
        "        mdf = sqlContext.sql(Query)\n",
        "        print(meas,': shape of mdf is ',mdf.count())\n",
        "\n",
        "        data=mdf.rdd.map(lambda row: unpackArray(row['Values'],np.float16))\n",
        "\n",
        "        #Compute basic statistics\n",
        "        STAT[meas]=computeOverAllDist(data)   # Compute the statistics \n",
        "\n",
        "        # compute covariance matrix\n",
        "        OUT=computeCov(data)\n",
        "\n",
        "        #find PCA decomposition\n",
        "        eigval,eigvec=LA.eig(OUT['Cov'])\n",
        "\n",
        "        # collect all of the statistics in STAT[meas]\n",
        "        STAT[meas]['eigval']=eigval\n",
        "        STAT[meas]['eigvec']=eigvec\n",
        "        STAT[meas].update(OUT)\n",
        "\n",
        "        print('time for',meas,'is',time()-t)\n",
        "    \n",
        "    return STAT\n",
        "\n",
        "# Compute the overall distribution of values and the distribution of the number of nan per year\n",
        "def find_percentiles(SortedVals,percentile):\n",
        "    L=int(len(SortedVals)/percentile)\n",
        "    return SortedVals[L],SortedVals[-L]\n",
        "  \n",
        "def computeOverAllDist(rdd0):\n",
        "    UnDef=np.array(rdd0.map(lambda row:sum(np.isnan(row))).sample(False,0.01).collect())\n",
        "    flat=rdd0.flatMap(lambda v:list(v)).filter(lambda x: not np.isnan(x)).cache()\n",
        "    count,S1,S2=flat.map(lambda x: np.float64([1,x,x**2]))\\\n",
        "                  .reduce(lambda x,y: x+y)\n",
        "    mean=S1/count\n",
        "    std=np.sqrt(S2/count-mean**2)\n",
        "    Vals=flat.sample(False,0.0001).collect()\n",
        "    SortedVals=np.array(sorted(Vals))\n",
        "    low100,high100=find_percentiles(SortedVals,100)\n",
        "    low1000,high1000=find_percentiles(SortedVals,1000)\n",
        "    return {'UnDef':UnDef,\\\n",
        "          'mean':mean,\\\n",
        "          'std':std,\\\n",
        "          'SortedVals':SortedVals,\\\n",
        "          'low100':low100,\\\n",
        "          'high100':high100,\\\n",
        "          'low1000':low100,\\\n",
        "          'high1000':high1000\n",
        "          }\n",
        "\n",
        "# description of data returned by computeOverAllDist\n",
        "STAT_Descriptions=[\n",
        "('SortedVals', 'Sample of values', 'vector whose length varies between measurements'),\n",
        " ('UnDef', 'sample of number of undefs per row', 'vector whose length varies between measurements'),\n",
        " ('mean', 'mean value', ()),\n",
        " ('std', 'std', ()),\n",
        " ('low100', 'bottom 1%', ()),\n",
        " ('high100', 'top 1%', ()),\n",
        " ('low1000', 'bottom 0.1%', ()),\n",
        " ('high1000', 'top 0.1%', ()),\n",
        " ('E', 'Sum of values per day', (365,)),\n",
        " ('NE', 'count of values per day', (365,)),\n",
        " ('Mean', 'E/NE', (365,)),\n",
        " ('O', 'Sum of outer products', (365, 365)),\n",
        " ('NO', 'counts for outer products', (365, 365)),\n",
        " ('Cov', 'O/NO', (365, 365)),\n",
        " ('Var', 'The variance per day = diagonal of Cov', (365,)),\n",
        " ('eigval', 'PCA eigen-values', (365,)),\n",
        " ('eigvec', 'PCA eigen-vectors', (365, 365))\n",
        "]\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing lib/tmp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS9RALI_Ehn8",
        "colab_type": "code",
        "outputId": "7d379889-7125-4f1b-bb39-618f5c790bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "%%time \n",
        "### This is the main cell, where all of the statistics are computed.\n",
        "STAT=computeStatistics(sqlContext,df)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TMAX : shape of mdf is  13437\n",
            "time for TMAX is 119.1714699268341\n",
            "SNOW : shape of mdf is  15629\n",
            "time for SNOW is 138.20228505134583\n",
            "SNWD : shape of mdf is  14617\n",
            "time for SNWD is 124.44392347335815\n",
            "TMIN : shape of mdf is  13442\n",
            "time for TMIN is 117.39819121360779\n",
            "PRCP : shape of mdf is  16118\n",
            "time for PRCP is 134.4644582271576\n",
            "TOBS : shape of mdf is  10956\n",
            "time for TOBS is 90.82817363739014\n",
            "CPU times: user 625 ms, sys: 512 ms, total: 1.14 s\n",
            "Wall time: 12min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVnWI2GlEqlU",
        "colab_type": "code",
        "outputId": "34444ea6-3e26-44a7-8b1f-90e2862da476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "print(\"   Name  \\t                 Description             \\t  Size\")\n",
        "print(\"-\"*80)\n",
        "print('\\n'.join([\"%10s\\t%40s\\t%s\"%(s[0],s[1],str(s[2])) for s in STAT_Descriptions]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Name  \t                 Description             \t  Size\n",
            "--------------------------------------------------------------------------------\n",
            "SortedVals\t                        Sample of values\tvector whose length varies between measurements\n",
            "     UnDef\t      sample of number of undefs per row\tvector whose length varies between measurements\n",
            "      mean\t                              mean value\t()\n",
            "       std\t                                     std\t()\n",
            "    low100\t                               bottom 1%\t()\n",
            "   high100\t                                  top 1%\t()\n",
            "   low1000\t                             bottom 0.1%\t()\n",
            "  high1000\t                                top 0.1%\t()\n",
            "         E\t                   Sum of values per day\t(365,)\n",
            "        NE\t                 count of values per day\t(365,)\n",
            "      Mean\t                                    E/NE\t(365,)\n",
            "         O\t                   Sum of outer products\t(365, 365)\n",
            "        NO\t               counts for outer products\t(365, 365)\n",
            "       Cov\t                                    O/NO\t(365, 365)\n",
            "       Var\t  The variance per day = diagonal of Cov\t(365,)\n",
            "    eigval\t                        PCA eigen-values\t(365,)\n",
            "    eigvec\t                       PCA eigen-vectors\t(365, 365)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPDT8N1HJHrS",
        "colab_type": "code",
        "outputId": "83a9b56f-bb52-4b11-a5d0-02310e85918f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "## Dump STAT and STST_Descriptions into a pickle file.\n",
        "from pickle import dump\n",
        "\n",
        "filename=data_dir+'/STAT_%s.pickle'%state\n",
        "dump((STAT,STAT_Descriptions),open(filename,'wb'))\n",
        "!ls -l $data_dir"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 89824\n",
            "drwxrwxr-x 2  498  500     4096 Apr 19  2018 NY.parquet\n",
            "-rw-r--r-- 1 root root 66288146 Mar 26 09:27 NY.tgz\n",
            "-rw-r--r-- 1 root root 25684418 Mar 26 09:40 STAT_NY.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}